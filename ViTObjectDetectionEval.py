import json
import random
from tqdm import tqdm
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.optim.lr_scheduler import StepLR
from transformers import ViTModel, ViTConfig
from PIL import Image
import os
from collections import defaultdict
import torch
from torch import nn
from torchvision.transforms import Compose
import matplotlib.patches as patches
from torchvision.transforms import ToTensor, Resize, Normalize
from torchvision.transforms import ToPILImage
from torchvision.ops import nms
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import warnings
warnings.filterwarnings('ignore')

model_name = 'google/vit-base-patch16-224'
config = ViTConfig.from_pretrained(model_name)
vit_backbone = ViTModel.from_pretrained(model_name, config=config)

class ObjectDetectionHead(nn.Module):
    def __init__(self, embedding_size, num_classes, num_boxes, hidden_layer_size=1024):
        super(ObjectDetectionHead, self).__init__()
        
        # Single fully connected layer
        self.fc_layer = nn.Sequential(
            nn.Linear(embedding_size, hidden_layer_size),
            nn.ReLU()  # Using ReLU for non-linearity
        )

        # Final layer for class predictions
        self.classifier = nn.Linear(hidden_layer_size, num_classes * num_boxes)

        # Final layer for bounding box regression
        self.box_regressor = nn.Linear(hidden_layer_size, 4 * num_boxes)

        # Activation function for bounding box coordinates
        self.box_regressor_activation = nn.Sigmoid()

    def forward(self, x):
        # Pass input through the fully connected layer
        x = self.fc_layer(x)

        # Class logits and bounding box coordinates
        class_logits = self.classifier(x)
        box_coordinates = self.box_regressor_activation(self.box_regressor(x))

        return class_logits, box_coordinates



class ObjectDetectionModel(nn.Module):
    def __init__(self, num_classes, num_boxes):
        super(ObjectDetectionModel, self).__init__()
        # Load the ViT model as a backbone
        config = ViTConfig.from_pretrained('google/vit-base-patch16-224')
        self.backbone = ViTModel.from_pretrained('google/vit-base-patch16-224', config=config)
        
        # The size of the embeddings generated by the backbone
        embedding_size = 768
        
        # Object detection head
        self.head = ObjectDetectionHead(embedding_size, num_classes, num_boxes)

    def forward(self, pixel_values):
        # Forward pass through the backbone
        outputs = self.backbone(pixel_values=pixel_values)
        
        # We use the representation of the [CLS] token (first token) for detection
        cls_token = outputs.last_hidden_state[:, 0, :]
        
        # Forward pass through the head
        class_logits, box_coordinates = self.head(cls_token)

        return class_logits, box_coordinates

    
class COCOParser:
    def __init__(self, anns_file, imgs_dir):
        with open(anns_file, 'r') as f:
            coco = json.load(f)

        self.annIm_dict = defaultdict(list)        
        self.cat_dict = {} 
        if 'categories' in coco:
            for cat in coco['categories']:
                self.cat_dict[cat['id']] = cat
        
        self.annId_dict = {}
        self.im_dict = {}
        self.licenses_dict = {}
        self.create_id_to_name_mapping()
        self.create_category_mapping()

        for ann in coco['annotations']:           
            self.annIm_dict[ann['image_id']].append(ann) 
            self.annId_dict[ann['id']]=ann
        for img in coco['images']:
            self.im_dict[img['id']] = img
        for cat in coco['categories']:
            self.cat_dict[cat['id']] = cat
        for license in coco['licenses']:
            self.licenses_dict[license['id']] = license

    def create_category_mapping(self):
        """
        Create a mapping from COCO category IDs to sequential indices.
        """
        all_cat_ids = sorted([cat_id for cat_id in self.cat_dict.keys()])
        self.cat_id_to_index = {cat_id: index for index, cat_id in enumerate(all_cat_ids)}
       # print("Category IDs in mapping:", sorted(self.cat_id_to_index.keys())) # debugging purposes

    def create_id_to_name_mapping(self):
        """
        Create a mapping from COCO category IDs to category names.
        """
        self.id_to_name = {cat['id']: cat['name'] for cat in self.cat_dict.values()}

    def get_imgIds(self):
        return list(self.im_dict.keys())

    def get_annIds(self, im_ids):
        im_ids=im_ids if isinstance(im_ids, list) else [im_ids]
        return [ann['id'] for im_id in im_ids for ann in self.annIm_dict[im_id]]
    
    def get_index_from_name(self, name):
        for cat_id, cat_name in self.id_to_name.items():
            if cat_name == name:
                return self.cat_id_to_index[cat_id]
        return -1  # Return an invalid index for unknown category names

    def load_anns(self, ann_ids):
        im_ids=ann_ids if isinstance(ann_ids, list) else [ann_ids]
        return [self.annId_dict[ann_id] for ann_id in ann_ids]        

    def load_cats(self, class_ids):
        class_ids=class_ids if isinstance(class_ids, list) else [class_ids]
        return [self.cat_dict[class_id] for class_id in class_ids]

    def get_imgLicenses(self,im_ids):
        im_ids=im_ids if isinstance(im_ids, list) else [im_ids]
        lic_ids = [self.im_dict[im_id]["license"] for im_id in im_ids]
        return [self.licenses_dict[lic_id] for lic_id in lic_ids]
    
    def load_bbox_and_labels(self, img_id):
        """
        Load bounding boxes and labels for a given image ID.
        """
        annotations = self.annIm_dict[img_id]
        bboxes = []
        labels = []

        for ann in annotations:
            # COCO bbox format: [x_min, y_min, width, height]
            bbox = ann['bbox']
            x, y, w, h = bbox
            bboxes.append([x, y, x + w, y + h])  # Convert to [x_min, y_min, x_max, y_max]
            labels.append(ann['category_id'])

        return bboxes, labels
    
class COCODataset(torch.utils.data.Dataset):
    def __init__(self, annotation_file, image_dir, transform=None):
        self.coco_parser = COCOParser(annotation_file, image_dir)
        self.transform = transform
        self.image_dir = image_dir

    def __len__(self):
        return len(self.coco_parser.get_imgIds())

    def __getitem__(self, idx):
        # Get image ID and path
        img_id = self.coco_parser.get_imgIds()[idx]
        img_path = os.path.join(self.image_dir, self.coco_parser.im_dict[img_id]['file_name'])

        # Load image
        image = Image.open(img_path).convert("RGB")
        original_size = image.size  # Original image size (W, H)


        # Load bounding boxes and labels
        bboxes, labels = self.coco_parser.load_bbox_and_labels(img_id)

        # Convert COCO bboxes from [x_min, y_min, w, h] to [x_min, y_min, x_max, y_max]
        bboxes = [[x, y, x + w, y + h] for x, y, w, h in bboxes]

        # Convert category IDs to names
        label_names = [self.coco_parser.id_to_name[label] for label in labels]

        # Map COCO category IDs to sequential indices
        labels = [self.coco_parser.cat_id_to_index[label] for label in labels]

        # Convert bboxes and labels to tensors
        bboxes = torch.as_tensor(bboxes, dtype=torch.float32)
        labels = torch.as_tensor(labels, dtype=torch.int64)

        target = {"image_id":img_id, "boxes": bboxes, "labels": label_names, "image_size": original_size}

        # Apply transformations to the image
        if self.transform:
            transformed_image = self.transform(image)

        # Adjust bounding boxes to the new image size after transformation
        transformed_size = transformed_image.size()  # New size after transformation (C, H, W)
        adjusted_bboxes = adjust_boxes(bboxes, original_size, (transformed_size[2], transformed_size[1]))
        target["boxes"] = adjusted_bboxes  # Update target with adjusted bboxes

        return transformed_image, target

transform = Compose([
    Resize((224, 224)),  # Resize images
    ToTensor(),
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

def collate_fn(batch):
    images = [item[0] for item in batch]  # Extract images
    targets = [item[1] for item in batch]  # Extract targets (dicts of 'boxes' and 'labels')

    images = torch.stack(images, dim=0)

    # since targets can have different numbers of objects,
    # we don't stack or pad them like images.
    return images, targets

def adjust_boxes(boxes, original_size, new_size):
    x_scale = new_size[0] / original_size[0]
    y_scale = new_size[1] / original_size[1]
    adjusted_boxes = []
    for box in boxes:
        x_min, y_min, x_max, y_max = box
        adjusted_boxes.append([
            x_min * x_scale, y_min * y_scale,
            x_max * x_scale, y_max * y_scale
        ])
    return adjusted_boxes

def post_process(batch):
    images = [item[0] for item in batch]  # Extract images
    targets = [item[1] for item in batch]  # Extract targets (dicts of 'boxes' and 'labels')

    images = torch.stack(images, dim=0)

    # since targets can have different numbers of objects,
    # we don't stack or pad them like images.
    return images, targets

def unnormalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):
    # Ensure mean and std have the correct shape
    mean = torch.tensor(mean, device=tensor.device).view(1, 3, 1, 1)
    std = torch.tensor(std, device=tensor.device).view(1, 3, 1, 1)
    
    tensor = tensor * std + mean  # unnormalize
    return tensor

def denormalize(image_tensor):
        mean = torch.tensor([0.485, 0.456, 0.406])
        std = torch.tensor([0.229, 0.224, 0.225])
        mean = mean.to(device)
        std = std.to(device)
        image_tensor = image_tensor * std[:, None, None] + mean[:, None, None]
        image_tensor = torch.clip(image_tensor, 0, 1)
        return image_tensor
'''''
def visualize_predictions(images, labels_list, boxes_list, scores_list, id_to_name_mapping, threshold=0.5):
    # Process each image in the batch
    for img, labels, boxes, scores in zip(images, labels_list, boxes_list, scores_list):
        print(img.shape)  # Add this line to debug the shape
        # Create a new figure for each image
        fig, ax = plt.subplots(1, figsize=(12, 12))
        
        # Unnormalize and plot the image
        #img = unnormalize(img)  # Assuming you have the unnormalize function defined as before
        img = img.permute(1, 2, 0).cpu().numpy()  # Permute to [H, W, C] for visualization
        ax.imshow(img)
        
        # Iterate over the boxes, labels, and scores for the current image
        for box, label, score in zip(boxes, labels, scores):
            # Ensure score is a single value and meets the threshold
            if torch.is_tensor(score):
                score = score.item()
            if score < threshold:
                continue

            # Convert the box to CPU and numpy array
            box = box.cpu().numpy()

            # Draw the bounding box and label
            rect = patches.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], linewidth=1, edgecolor='r', facecolor='none')
            ax.add_patch(rect)
            label_name = id_to_name_mapping.get(label.item() if torch.is_tensor(label) else label, 'Unknown')
            ax.text(box[0], box[1], f'{label_name}: {score:.2f}', bbox=dict(facecolor='yellow', alpha=0.5), fontsize=8, color='black')

        ax.axis('off')
        plt.show()  # Show each image in a separate plot
        '''
def visualize_predictions(image_tensor, boxes, scores, image_size):
    # Convert image tensor to PIL Image and then to a NumPy array
    image = ToPILImage()(image_tensor)
    image = np.array(image)

    # Ensure the image is in the correct format for visualization (HxWxC)
    if image.shape[-1] != 3:  # Check for RGB channels
        image = image.transpose(1, 2, 0)  # Transpose to HxWxC

    fig, ax = plt.subplots(1)
    ax.imshow(image)

    for box, score in zip(boxes, scores):
        # Ensure 'box' is a list or array with 4 elements
        if not isinstance(box, (list, np.ndarray)) or len(box) != 4:
            continue  # Skip invalid box entries

        # Draw the rectangle
        rect = patches.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1],
                                 linewidth=2, edgecolor='red', facecolor='none')
        ax.add_patch(rect)

        # Add the score text
        ax.text(box[0], box[1], f"{score:.2f}", fontsize=12, color='white',
                bbox=dict(facecolor='yellow', alpha=0.75))

    plt.axis('off')
    plt.show()
    plt.savefig("/home/ps332/myViT/visualize_predictionsCORRECT.png")

"""""
def visualize_batch_predictions(images, class_logits, box_coordinates, threshold=0.8):
    batch_size = images.size(0)
    for i in range(batch_size):
        image_tensor = images[i]  # Get the image tensor
        logits = class_logits[i]  # Get the logits for the current image
        boxes = box_coordinates[i].view(-1, 4)  # Get the bboxes for the current image
        
        scores = torch.sigmoid(logits).detach().cpu()  # Convert logits to probabilities and move to cpu
        scores, indices = torch.max(scores, dim=1)  # Get the max score for each box
        keep_boxes = scores > threshold  # Filter boxes by score threshold
        
        if keep_boxes.sum() > 0:  
            valid_boxes = boxes[keep_boxes].detach().cpu().numpy()
            valid_scores = scores[keep_boxes].detach().cpu().numpy()
            image_size = image_tensor.shape[1:]  # HxW
            print("image size", image_size)
            denormalized_image = denormalize(image_tensor).cpu()
            # Apply NMS
            final_boxes, final_scores = apply_nms(valid_boxes, valid_scores)

            # Visualize the final predictions after NMS
            visualize_predictions(denormalized_image, final_boxes, final_scores, image_size)
"""

def visualize_batch_predictions(images, class_logits, scaled_bboxes_list, threshold=0.8):
    batch_size = images.size(0)
    for i in range(batch_size):
        image_tensor = images[i]  # Get the image tensor
        logits = class_logits[i]  # Get the logits for the current image
        scaled_bboxes = scaled_bboxes_list[i]  # Get the scaled bboxes for the current image
        
        scores = torch.sigmoid(logits).detach().cpu()  # Convert logits to probabilities and move to cpu
        scores, indices = torch.max(scores, dim=1)  # Get the max score for each box
        keep_boxes = scores > threshold  # Filter boxes by score threshold

        valid_boxes = [scaled_bboxes[j] for j in range(len(scaled_bboxes)) if keep_boxes[j]]  # Select valid boxes
        valid_scores = scores[keep_boxes].numpy()  # Select corresponding scores

        if len(valid_boxes) > 0:
            image_size = image_tensor.shape[1:]  # HxW
            denormalized_image = denormalize(image_tensor).cpu()
            
            # Apply NMS (if you have this function)
            #final_boxes, final_scores = apply_nms(valid_boxes, valid_scores)

            # Visualize the final predictions after NMS
            visualize_predictions(denormalized_image, valid_boxes, valid_scores, image_size)

def apply_nms(orig_boxes, orig_scores, iou_thresh=0.85):
    # Convert to torch tensors
    boxes = torch.as_tensor(orig_boxes, dtype=torch.float32)
    scores = torch.as_tensor(orig_scores, dtype=torch.float32)

    # Apply NMS
    keep_indices = nms(boxes, scores, iou_thresh)

    # Convert back to numpy
    final_boxes = boxes[keep_indices].numpy()
    final_scores = scores[keep_indices].numpy()

    return final_boxes, final_scores


def process_model_output(class_logits, bbox_coords, threshold=0.85):
    num_classes = 80
    num_boxes = 100  # Number of boxes per image
    batch_size = class_logits.shape[0]

    # Reshape logits to [batch_size, num_boxes, num_classes]
    reshaped_logits = class_logits.view(batch_size, num_boxes, num_classes)
    #print("reshaped logits", reshaped_logits)

    # Convert logits to probabilities using sigmoid
    probabilities = torch.sigmoid(reshaped_logits)
    #print("probs", probabilities)

    # Choose the label with the highest probability for each box
    max_probs, labels = torch.max(probabilities, dim=2)
    #print("max_probs", max_probs)
    ################################################################^^^correct up to here

    # Initialize lists to store the results for each image
    all_selected_labels = []
    all_selected_boxes = []
    all_selected_scores = []

    # Reshape bbox_coords to match the logits shape for easy indexing
    reshaped_bbox_coords = bbox_coords.view(batch_size, num_boxes, 4)

    for i in range(batch_size):
        # Select boxes, labels, and scores for this image that exceed the threshold
        img_selected_indices = max_probs[i] >= threshold
        img_selected_labels = labels[i][img_selected_indices]
        img_selected_boxes = reshaped_bbox_coords[i][img_selected_indices]
        img_selected_scores = max_probs[i][img_selected_indices]

        # Add to the lists
        all_selected_labels.append(img_selected_labels)
        all_selected_boxes.append(img_selected_boxes)
        all_selected_scores.append(img_selected_scores)

    return all_selected_labels, all_selected_boxes, all_selected_scores

def convert_and_normalize_bbox(bbox, image_width, image_height):
    x, y, width, height = bbox
    x_min = x / image_width
    y_min = y / image_height
    x_max = (x + width) / image_width
    y_max = (y + height) / image_height
    return [x_min, y_min, x_max, y_max]


def scale_coords_to_pixels(bboxes, original_width, original_height):
    scaled_bboxes = []
    for bbox in bboxes:
        x_min, y_min, x_max, y_max = bbox
        scaled_bboxes.append([
            x_min * original_width, y_min * original_height,
            x_max * original_width, y_max * original_height
        ])
    return scaled_bboxes

def scale_boxes_to_original(boxes, original_width, original_height):
    scaled_boxes = []
    for box in boxes:
        x_min, y_min, x_max, y_max = box
        scaled_box = [
            x_min * original_width, y_min * original_height,
            x_max * original_width, y_max * original_height
        ]
        scaled_boxes.append(scaled_box)
    return scaled_boxes



# Device selection (CUDA GPU if available, otherwise CPU)
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)

"""""
ToDo:
1. Reshape logits (currently 1x8000) to 80x100
2. Choose label with highest probablity for each box resulting in a 1x100 tensor
3. Apply threshold to choose best boxes - 1xn tensor
4. Visualize
5. Compile into result file with data format {image_id, cat_id, bbox, score} for each selected box
-Format: [{ "image_id" : int, "category_id" : int, "bbox" : [x,y,width,height], "score" : float, }]
-Convert box_coordinates to bbox
-Evaluate with cocoeval
"""""

def main():
    num_classes=80
    random_seed = 42
    num_boxes = 100  
    torch.manual_seed(random_seed)
    random.seed(random_seed)
   
    eval_dataset = COCODataset(annotation_file="/home/ps332/myViT/coco_ann2017/annotations/instances_val2017.json",
                               image_dir= "/home/ps332/myViT/coco_val2017/val2017",
                               transform = transform)
    eval_loader = DataLoader(eval_dataset, batch_size = 2, shuffle = False, collate_fn = collate_fn )
    
    model = ObjectDetectionModel(num_classes=num_classes, num_boxes=num_boxes)

    model.load_state_dict(torch.load("/home/ps332/myViT/object_detection_model_final.pth"))
    model.to(device)  
    model.eval()

    with torch.no_grad():
        for imgs, targets in tqdm(eval_loader):
            images = imgs.to(device)
            batch_size = 2
            num_predicted_boxes = 100
            num_classes=80
            threshold = 0.8

            #Forward pass to get logits and box_coordinates for the entire batch
            class_logits, box_coordinates = model(images)
            print(f"box coords for image:", box_coordinates)


            #Reshape the predictions
            class_logits = class_logits.view(batch_size, num_predicted_boxes, num_classes)
            box_coordinates = box_coordinates.view(batch_size, num_predicted_boxes, 4)

            
            
            # Process each image in the batch
            #for i in range(batch_size):
            # Get the original image size (assuming you have a way to get this information)
             #   original_width, original_height = targets[i]['image_size']

                # Assuming original_width and original_height are the dimensions of the input image
              #  scaled_boxes = scale_boxes_to_original(box_coordinates, original_width, original_height)
               # print("scaled bboxes", scaled_boxes) #THIS IS CORRECT AND WORKING


            # Scale the coordinates
                #scaled_bboxes = scale_coords_to_pixels(box_coordinates[i].cpu().numpy(), original_width, original_height)
                #print(f"Scaled Bounding Boxes for image {i}:", scaled_bboxes)
            
             # Check if there are any predicted boxes
                #if len(scaled_bboxes) == 0:
                  #  print(f"No bounding boxes predicted for image {i} with threshold {threshold}")
                   # continue

            #Visualize predictions for the batch
                #visualize_batch_predictions(images, class_logits, scaled_bboxes, threshold=0.5)


if __name__ == '__main__':
    main()            
